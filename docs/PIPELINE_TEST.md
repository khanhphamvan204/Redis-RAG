# ğŸ§ª Test Kafka-Spark-MongoDB Pipeline

## âœ… Spark Ä‘Ã£ start - Giá» test pipeline!

### ğŸ“ Test Plan:

1. **Generate test queries** â†’ FastAPI ghi log vÃ  publish Kafka
2. **Kafka receives** â†’ Messages trong topic `user-queries`
3. **Spark consumes** â†’ Real-time aggregation
4. **MongoDB receives** â†’ Data trong analytics collections
5. **Verify** â†’ Check MongoDB cÃ³ data

---

## ğŸš€ BÆ°á»›c 1: Generate Test Queries

### Option A: DÃ¹ng Chat UI (Khuyáº¿n nghá»‹)

```
1. Má»Ÿ http://localhost:3000
2. Login
3. Há»i 5-10 cÃ¢u khÃ¡c nhau
4. Má»—i cÃ¢u sáº½ tá»± Ä‘á»™ng:
   - Log vÃ o MongoDB `query_logs`
   - Publish lÃªn Kafka `user-queries`
```

**CÃ¢u há»i gá»£i Ã½**:

- "ThÃ´ng tin vá» khoa CNTT"
- "Lá»‹ch há»c hÃ´m nay"
- "Äiá»ƒm thi cuá»‘i ká»³"
- "GiÃ¡o viÃªn khoa ToÃ¡n"
- "Quy Ä‘á»‹nh vá» há»c phÃ­"

### Option B: Script Python (Náº¿u UI khÃ´ng dÃ¹ng Ä‘Æ°á»£c)

```python
# test_query_generation.py
import requests

API_URL = "http://localhost:8000/documents/vector/search-with-llm-context"
TOKEN = "YOUR_JWT_TOKEN"  # Get from localStorage in browser

queries = [
    "ThÃ´ng tin vá» khoa CNTT",
    "Lá»‹ch há»c hÃ´m nay",
    "Äiá»ƒm thi cuá»‘i ká»³",
    "GiÃ¡o viÃªn khoa ToÃ¡n",
    "Quy Ä‘á»‹nh vá» há»c phÃ­"
]

headers = {"Authorization": f"Bearer {TOKEN}"}

for query in queries:
    payload = {
        "query": query,
        "file_type": "public",
        "k": 5,
        "session_id": "test_session_123"
    }

    response = requests.post(API_URL, json=payload, headers=headers)
    print(f"âœ“ Query: {query} - Status: {response.status_code}")
```

---

## ğŸ” BÆ°á»›c 2: Verify Kafka Received Messages

```bash
# Check topic message count (via Kafka UI)
# URL: http://localhost:8080
# Navigate to: Topics â†’ user-queries â†’ Messages

# OR check via CLI
docker exec kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic user-queries \
  --from-beginning \
  --max-messages 5
```

**Expected**: JSON messages vá»›i query data

---

## âš™ï¸ BÆ°á»›c 3: Wait for Spark Processing

Spark processes data theo window:

- **Faculty aggregation**: 5-minute window, 1-minute slide
- **Year aggregation**: 5-minute window, 1-minute slide
- **Heatmap**: 15-minute window, 5-minute slide

**Äá»£i**: ~2-3 phÃºt sau khi generate queries

---

## ğŸ“Š BÆ°á»›c 4: Check MongoDB Analytics Collections

### Run Check Script:

```bash
cd "E:\HK1 (2025 - 2026)\BigData\rag\redis_rag"
python scripts/check_mongodb_data.py
```

### Expected Output:

```
ğŸ“Š Collection: query_logs
   âœ… Found 15 documents  # Raw queries

ğŸ“Š Collection: query_analytics_by_faculty
   âœ… Found 3 documents   # Aggregated by faculty

   Sample document:
   {
     "window_start": "2025-12-08T11:00:00",
     "window_end": "2025-12-08T11:05:00",
     "faculty": "CNTT",
     "query_count": 5,
     "unique_users": 2,
     "avg_response_time": 1234.5,
     "avg_contexts_found": 4.2
   }

ğŸ“Š Collection: query_analytics_by_year
   âœ… Found 2 documents   # Aggregated by year

ğŸ“Š Collection: query_analytics_heatmap
   âœ… Found 4 documents   # Faculty x Year heatmap
```

---

## âœ… Success Criteria:

- [x] Chat UI hoáº¡t Ä‘á»™ng
- [x] Queries Ä‘Æ°á»£c log vÃ o `query_logs`
- [x] Kafka topic `user-queries` cÃ³ messages
- [x] Spark logs show "Batch X written to MongoDB"
- [x] Collections `query_analytics_*` cÃ³ data

---

## ğŸ› Troubleshooting:

### âŒ Queries khÃ´ng vÃ o Kafka:

```bash
# Check FastAPI logs
# TÃ¬m: "Query tracked" vÃ  "Publishing to Kafka"

# Náº¿u khÃ´ng tháº¥y "Publishing to Kafka"
# â†’ Check kafka_service.py import errors
```

### âŒ Spark khÃ´ng ghi MongoDB:

```bash
# Check Spark logs
docker logs spark-streaming | findstr "MongoDB"

# Common errors:
# - "Connection refused" â†’ MongoDB khÃ´ng accessible tá»« container
# - "Authentication failed" â†’ Sai credentials
```

### âŒ Collections váº«n rá»—ng after 5 minutes:

```bash
# 1. Check Spark cÃ³ Ä‘ang process khÃ´ng
docker logs spark-streaming | findstr "Batch"

# 2. Check window timing
# Spark chá»‰ flush data khi window káº¿t thÃºc
# Náº¿u vá»«a generate queries, Ä‘á»£i thÃªm 5 phÃºt

# 3. Manually trigger checkpoint
docker-compose -f docker-compose.kafka.yml restart spark-streaming
```

---

## ğŸ“ˆ Next Steps After Success:

1. âœ… Pipeline verified â†’ Create Superset dashboards
2. âœ… Dashboards created â†’ Get Dashboard IDs
3. âœ… Update IDs in `AnalyticsView.jsx`
4. âœ… Test embedding in React UI
5. âœ… Done! ğŸ‰

---

**Good luck testing!** ğŸš€
