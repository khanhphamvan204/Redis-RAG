# Spark Streaming Setup Guide

Apache Spark Streaming cho real-time analytics tá»« Kafka.

## ğŸš€ Khá»Ÿi Ä‘á»™ng Spark Streaming

### Start toÃ n bá»™ stack (Kafka + Spark)

```bash
# Build vÃ  start táº¥t cáº£ services
docker-compose -f docker-compose.kafka.yml up -d --build

# Xem logs cá»§a Spark
docker logs spark-streaming -f

# Check status
docker ps | grep spark
```

### Stop services

```bash
docker-compose -f docker-compose.kafka.yml down

# XÃ³a cáº£ volumes vÃ  checkpoints
docker-compose -f docker-compose.kafka.yml down -v
```

## ğŸ“Š Monitoring & UI

### 1. Spark UI

- **URL**: http://localhost:4040
- **Features**:
  - Streaming tab: Active streaming queries
  - SQL tab: Running aggregations
  - Jobs/Stages: Execution details
  - Executors: Resource usage

### 2. Kafka UI

- **URL**: http://localhost:8080
- **Monitor**:
  - Topic `user-queries` messages
  - Consumer group lag
  - Partition distribution

### 3. MongoDB

Connect Ä‘á»ƒ xem káº¿t quáº£ analytics:

```bash
mongosh "mongodb://admin:123@localhost:27017/faiss_db?authSource=admin"
```

Queries:

```javascript
// Xem analytics by faculty
db.query_analytics_by_faculty.find().sort({ window_start: -1 }).limit(5);

// Xem analytics by year
db.query_analytics_by_year.find().sort({ window_start: -1 }).limit(5);

// Xem heatmap data
db.query_analytics_heatmap.find().sort({ window_start: -1 }).limit(5);
```

## ğŸ” Architecture Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User Query â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI App   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
     â”‚        â”‚
     â–¼        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MongoDB â”‚  â”‚ Kafka Topic  â”‚
â”‚ (Logs)  â”‚  â”‚ user-queries â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Spark Streaming  â”‚
          â”‚ (3 Aggregations) â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚    MongoDB      â”‚
          â”‚  (Analytics)    â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â€¢ by_faculty
            â€¢ by_year
            â€¢ heatmap
```

## ğŸ“ˆ Streaming Aggregations

### 1. **By Faculty** (5-min window, 1-min slide)

Metrics:

- Query count
- Unique users
- Avg response time
- Avg contexts found
- Rewritten queries count
- History usage count

### 2. **By Year** (5-min window, 1-min slide)

Metrics:

- Query count per year
- Unique users
- Avg performance metrics

### 3. **Heatmap** (15-min window, 5-min slide)

Cross-analysis:

- Faculty Ã— Year distribution
- Query patterns

## ğŸ§ª Testing

### Test End-to-End Pipeline

```bash
# 1. Äáº£m báº£o táº¥t cáº£ services Ä‘ang cháº¡y
docker ps

# Expected containers:
# - zookeeper
# - kafka
# - kafka-ui
# - spark-streaming
# - mongo-db (tá»« docker-compose.yml khÃ¡c)

# 2. Submit test query qua FastAPI
# (Use Postman/curl hoáº·c qua UI)

# 3. Check Kafka message
docker exec -it kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic user-queries \
  --from-beginning \
  --max-messages 1

# 4. Monitor Spark processing
docker logs spark-streaming --tail 50

# 5. Wait 5-15 phÃºt (window time)

# 6. Query MongoDB cho analytics
mongosh "mongodb://admin:123@localhost:27017/faiss_db?authSource=admin"
```

## âš™ï¸ Configuration

### Environment Variables (.env)

```env
# Kafka
KAFKA_BROKER_URL=localhost:9092
KAFKA_TOPIC_QUERIES=user-queries

# MongoDB (for analytics)
DATABASE_URL=mongodb://admin:123@host.docker.internal:27017/faiss_db?authSource=admin
```

### Spark Resources

Máº·c Ä‘á»‹nh trong `docker-compose.kafka.yml`:

- Driver Memory: 2GB
- Executor Memory: 2GB
- Master: local[2] (2 cores)

Äá»ƒ thay Ä‘á»•i, edit `spark/entrypoint.sh`:

```bash
spark-submit \
    --conf spark.driver.memory=4g \
    --conf spark.executor.memory=4g \
    ...
```

## ğŸ› Troubleshooting

### Spark container khÃ´ng start

**Check logs:**

```bash
docker logs spark-streaming
```

**Common issues:**

1. **Kafka not ready**: Spark Ä‘á»£i Kafka 30s, náº¿u khÃ´ng connect Ä‘Æ°á»£c sáº½ fail

   - Solution: Äáº£m báº£o Kafka healthy trÆ°á»›c khi start Spark

2. **MongoDB connection error**:

   - Check MongoDB Ä‘ang cháº¡y: `docker ps | grep mongo`
   - Verify credentials trong MONGO_URI

3. **Port 4040 conflict**:
   - Change port trong `docker-compose.kafka.yml`: `"4041:4040"`

### No analytics data in MongoDB

**Possible reasons:**

1. **No query events**: ChÆ°a cÃ³ query nÃ o qua há»‡ thá»‘ng

   - Submit queries qua FastAPI

2. **Window chÆ°a complete**: Aggregation windows cáº§n thá»i gian (5-15 phÃºt)

   - Äá»£i thÃªm vÃ  check láº¡i

3. **Spark job error**: Check Spark logs
   ```bash
   docker logs spark-streaming | grep ERROR
   ```

### High memory usage

```bash
# Monitor resources
docker stats spark-streaming

# Reduce memory in entrypoint.sh
--conf spark.driver.memory=1g \
--conf spark.executor.memory=1g
```

## ğŸ“ Logs Location

```bash
# Spark application logs
docker logs spark-streaming

# Spark checkpoints (persisted)
docker volume inspect redis_rag_spark-checkpoints
```

## ğŸ”„ Restart Streaming Job

```bash
# Restart container
docker restart spark-streaming

# Rebuild náº¿u code thay Ä‘á»•i
docker-compose -f docker-compose.kafka.yml up -d --build spark-streaming

# Clear checkpoints Ä‘á»ƒ start fresh
docker-compose -f docker-compose.kafka.yml down -v
docker-compose -f docker-compose.kafka.yml up -d --build
```

## ğŸ“š Related Documentation

- [Kafka Setup Guide](KAFKA_SETUP.md)
- [HDFS Setup Guide](HDFS_SETUP.md)
